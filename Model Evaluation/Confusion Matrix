###### Confusion Matrix
# False Negative, False Positive, True Positive, True Negative

# for code reference
x = df.drop('actualhospitalmortality', axis=1)
y = df['actualhospitalmortality']

x_train = 
y_train = 

x_test = 
y_test = 

y_hat_train = reg.predict(x_train)
y_hat_test = reg.predict(x_test)

# import the metrics class
from sklearn import metrics
confusion = metrics.confusion_matrix(y_test, y_hat_test)

class_names=df['actualhospitalmortality'].cat.categories
disp = metrics.ConfusionMatrixDisplay.from_estimator(
    reg, x_test, y_test, display_labels=class_names,
    cmap=plt.cm.Blues)

plt.show()

##### Accuracy Test
#Accuracy is the overall proportion of correct predictions. 
# ACcuracy =  (True Positive + True Negative) / (True Positive + True Negative + False Positive + False Negative)

acc = metrics.accuracy_score(y_test, y_hat_test)
print(f"Accuracy (model) = {acc:.2f}")


########### Sensitivity/ Recall (True Positive Rate)
# The ability of the algorithm to predict a positive outcome when the actual outcome is positive
# True Positive / (True Positive + False Negative)

########## Specificity (True Negative Rate)
# True Negative / (False Positive + True Negative)

######## ROC Curve
# Receiver - Operator Characteristic - plots specificity vs sensitivity at varying probability thresholds.
# AUROC stands for Area under the ROC
# AUROC score of .5 is no better than guessing, 1.0 is perfect
metrics.plot_roc_curve(reg, x_test, y_test)
